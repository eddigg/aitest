## 🚀 Rapid Execution Plan – Human Idea to Computer Execution System

As the head of the Development Department, I’m providing you, the Senior Developer, with a clear, concise, and purely technical plan to execute the development of a system that translates human ideas into executable actions via voice, video, and text inputs, using machine learning. The goal is to deliver a functional Minimal Viable Product (MVP) by the end of the day, April 17, 2025 (hypothetically a single workday for this exercise). This plan ensures zero room for error in execution, focusing on voice input for the MVP while laying the groundwork for video and text inputs. Below are your detailed instructions.

---

### 🎯 Objective
Develop an MVP that:
1. Accepts voice inputs.
2. Transcribes them to text.
3. Classifies intents.
4. Maps them to predefined actions.
5. Executes simulated actions with feedback via a simple UI.

---

### 📁 Project Structure
Create the following directory structure to keep the project modular and extensible:

```
/project_root/
├── voice_recognition/
│   ├── transcribe.py       # Speech-to-Text with Whisper
│   └── classify.py        # Intent classification
├── command_mapping/
│   ├── action_map.py      # Intent-to-action mapping
├── event_handling/
│   ├── execute.py         # Action execution logic
├── ui/
│   ├── static/
│   │   ├── index.html     # Frontend UI
│   │   └── script.js      # Audio recording and API calls
│   └── templates/         # Optional for FastAPI templates (if needed)
├── main.py                # FastAPI backend
├── requirements.txt       # Dependencies
└── logs/                  # For log files
```

---

### 🛠️ Tools & Libraries
Use the following tools, all open-source and free for this MVP, to avoid costs:

| **Component**           | **Tool/Library**             | **Purpose**                     | **Installation**                  |
|--------------------------|------------------------------|----------------------------------|------------------------------------|
| Backend                 | FastAPI, Uvicorn            | API server                      | `pip install fastapi uvicorn`     |
| Speech-to-Text          | OpenAI Whisper (via Transformers) | Transcribe voice input    | `pip install transformers torch`  |
| Intent Classification   | scikit-learn                | Classify intents               | `pip install scikit-learn`        |
| Audio Processing        | pydub                       | Handle audio formats           | `pip install pydub`               |
| Logging                 | Python `logging`            | Structured logs                | Built-in                          |
| Frontend                | HTML, JavaScript, MediaRecorder | Record and send audio       | No install needed                 |

---

### ✅ Deliverables by EOD
1. A working `main.py` that integrates all components and processes voice input end-to-end.
2. Voice recognition module transcribing audio and classifying intents.
3. Command mapping and event handling simulating actions (e.g., print statements).
4. A basic UI showing transcribed text and action feedback.
5. Stubs for video and text inputs (placeholders for future expansion).

---

### 🛠️ Step-by-Step Instructions

#### 1. Project Setup
- **Task**: Initialize the project environment.
- **Instructions**:
  1. Create a new Python virtual environment: `python -m venv venv`.
  2. Activate it: `source venv/bin/activate` (Linux/Mac) or `venv\Scripts\activate` (Windows).
  3. Install dependencies: Copy the `requirements.txt` below and run `pip install -r requirements.txt`.
  4. Create the directory structure as outlined above.

- **`requirements.txt`**:
```
fastapi==0.115.0
uvicorn==0.30.6
transformers==4.44.2
torch==2.4.1
scikit-learn==1.5.2
pydub==0.25.1
```

---

#### 2. Voice Recognition Module (`voice_recognition/`)
- **Task**: Implement speech-to-text and intent classification.
- **Instructions**:

  **a. `transcribe.py`**:
  - Implement a function to transcribe audio using Whisper.
  - File: `voice_recognition/transcribe.py`
  ```python
  from transformers import WhisperProcessor, WhisperForConditionalGeneration
  from pydub import AudioSegment
  import io

  processor = WhisperProcessor.from_pretrained("openai/whisper-small")
  model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

  def transcribe_audio(audio_data: bytes) -> str:
      # Convert audio to WAV if needed
      audio = AudioSegment.from_file(io.BytesIO(audio_data))
      audio = audio.set_frame_rate(16000).set_channels(1)  # Whisper expects 16kHz mono
      audio.export("temp.wav", format="wav")

      # Process audio
      input_features = processor(audio.get_array_of_samples(), sampling_rate=16000, return_tensors="pt").input_features
      predicted_ids = model.generate(input_features)
      transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
      return transcription
  ```

  **b. `classify.py`**:
  - Use TF-IDF and logistic regression to classify intents from text.
  - Prepare a small dataset inline and train the classifier.
  - File: `voice_recognition/classify.py`
  ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.linear_model import LogisticRegression

  # Training data (hardcoded for MVP)
  commands = [
      ("turn on the lights", "turn_on_lights"),
      ("switch on lights", "turn_on_lights"),
      ("turn off the lights", "turn_off_lights"),
      ("switch off lights", "turn_off_lights"),
      ("play music", "play_music"),
      ("start music", "play_music"),
      ("stop music", "stop_music"),
      ("pause music", "stop_music"),
      ("hello there", "unknown")
  ]
  texts, intents = zip(*commands)

  # Train classifier
  vectorizer = TfidfVectorizer()
  X = vectorizer.fit_transform(texts)
  clf = LogisticRegression()
  clf.fit(X, intents)

  def classify_intent(text: str) -> str:
      X_test = vectorizer.transform([text])
      intent = clf.predict(X_test)[0]
      confidence = clf.predict_proba(X_test).max()
      return intent if confidence > 0.5 else "unknown"
  ```

---

#### 3. Command Mapping Module (`command_mapping/`)
- **Task**: Map classified intents to actions.
- **Instructions**:
  - File: `command_mapping/action_map.py`
  ```python
  def turn_on_lights():
      print("Lights turned on")

  def turn_off_lights():
      print("Lights turned off")

  def play_music():
      print("Music playing")

  def stop_music():
      print("Music stopped")

  action_map = {
      "turn_on_lights": turn_on_lights,
      "turn_off_lights": turn_off_lights,
      "play_music": play_music,
      "stop_music": stop_music
  }

  def get_action(intent: str):
      return action_map.get(intent)
  ```

---

#### 4. Event Handling Module (`event_handling/`)
- **Task**: Execute actions and handle errors.
- **Instructions**:
  - File: `event_handling/execute.py`
  ```python
  import logging

  logging.basicConfig(filename="logs/app.log", level=logging.INFO, format="%(asctime)s - %(message)s")

  def execute_action(intent: str, action):
      if action:
          try:
              action()
              logging.info(f"Executed intent: {intent}")
              return {"status": "success", "message": f"Command '{intent}' executed"}
          except Exception as e:
              logging.error(f"Error executing {intent}: {e}")
              return {"status": "error", "message": str(e)}
      logging.warning(f"Unknown intent: {intent}")
      return {"status": "unknown_command", "message": "Command not recognized"}
  ```

---

#### 5. Backend Integration (`main.py`)
- **Task**: Create a FastAPI backend to tie everything together.
- **Instructions**:
  - File: `main.py`
  ```python
  from fastapi import FastAPI, UploadFile, File
  from voice_recognition.transcribe import transcribe_audio
  from voice_recognition.classify import classify_intent
  from command_mapping.action_map import get_action
  from event_handling.execute import execute_action

  app = FastAPI()

  @app.post("/process_audio")
  async def process_audio(audio: UploadFile = File(...)):
      audio_data = await audio.read()
      transcription = transcribe_audio(audio_data)
      intent = classify_intent(transcription)
      action = get_action(intent)
      result = execute_action(intent, action)
      return {"transcription": transcription, "intent": intent, **result}
  ```

- **Run the server**: `uvicorn main:app --reload`

---

#### 6. UI Setup (`ui/`)
- **Task**: Build a simple frontend to record audio and display results.
- **Instructions**:

  **a. `ui/static/index.html`**:
  ```html
  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8">
      <title>Voice Command System</title>
  </head>
  <body>
      <button id="record">Record Command</button>
      <p>Transcription: <span id="transcription"></span></p>
      <p>Intent: <span id="intent"></span></p>
      <p>Result: <span id="result"></span></p>
      <script src="script.js"></script>
  </body>
  </html>
  ```

  **b. `ui/static/script.js`**:
  ```javascript
  const recordButton = document.getElementById('record');
  let mediaRecorder;
  let audioChunks = [];

  recordButton.addEventListener('click', async () => {
      if (!mediaRecorder || mediaRecorder.state === 'inactive') {
          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          mediaRecorder = new MediaRecorder(stream);
          audioChunks = [];

          mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);
          mediaRecorder.onstop = sendAudio;
          mediaRecorder.start();
          recordButton.textContent = 'Stop Recording';
      } else {
          mediaRecorder.stop();
          mediaRecorder.stream.getTracks().forEach(track => track.stop());
          recordButton.textContent = 'Record Command';
      }
  });

  async function sendAudio() {
      const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
      const formData = new FormData();
      formData.append('audio', audioBlob, 'command.webm');

      const response = await fetch('http://localhost:8000/process_audio', {
          method: 'POST',
          body: formData
      });
      const result = await response.json();

      document.getElementById('transcription').textContent = result.transcription;
      document.getElementById('intent').textContent = result.intent;
      document.getElementById('result').textContent = `${result.status}: ${result.message}`;
  }
  ```

- **Serve static files**: Update `main.py` to include:
  ```python
  from fastapi.staticfiles import StaticFiles
  app.mount("/static", StaticFiles(directory="ui/static"), name="static")
  ```

---

#### 7. Testing
- **Task**: Verify the system works end-to-end.
- **Instructions**:
  1. Start the server: `uvicorn main:app --reload`.
  2. Open `http://localhost:8000/static/index.html` in a browser.
  3. Record commands like "turn on the lights", "play music", etc.
  4. Check:
     - Transcription appears correctly.
     - Intent matches the command (or "unknown" if not recognized).
     - Action result shows "success" or appropriate error.
  5. Verify logs in `logs/app.log` for each command.

---

### 📝 Notes
- **Focus**: Voice input only for MVP. Video and text stubs can be added as empty functions in `main.py` (e.g., `@app.post("/process_video")` returning `{"status": "not_implemented"}`).
- **Simulation**: Actions are simulated with `print()` statements. Replace with API calls in future iterations.
- **Error Handling**: Basic exceptions are caught and logged. Expand with retries if needed later.
- **Scalability**: This runs locally. For deployment, consider Docker and model optimization (e.g., TensorFlow Lite) later.

---

### ⏰ Time Allocation (10-hour workday)
- Setup: 2 hours
- Voice Recognition: 3 hours
- Command Mapping: 1 hour
- Event Handling: 1 hour
- UI & Integration: 2 hours
- Testing: 1 hour

Execute these tasks precisely as outlined, and we’ll meet the deadline with a functional MVP. Report any blockers immediately. Let’s make this a success!